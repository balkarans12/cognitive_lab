{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LAB ASSIGNMENT 9"
      ],
      "metadata": {
        "id": "kOMhutNHy4wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 1\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"\"\"Technology has revolutionized the way we live, work, and communicate.\n",
        "From smartphones to artificial intelligence, advancements in technology continue to shape our future.\n",
        "The rapid pace of innovation has brought convenience and efficiency to our daily lives.\n",
        "However, it also raises ethical concerns and challenges that must be addressed.\"\"\"\n",
        "\n",
        "text_cleaned = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "words = word_tokenize(text_cleaned)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "print(\"Word Frequency Distribution (Excluding Stopwords):\")\n",
        "print(word_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlidAmaNyJ0M",
        "outputId": "ee6bb144-532d-4e0c-dee6-6cff57b50bdd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency Distribution (Excluding Stopwords):\n",
            "Counter({'technology': 2, 'revolutionized': 1, 'way': 1, 'live': 1, 'work': 1, 'communicate': 1, 'smartphones': 1, 'artificial': 1, 'intelligence': 1, 'advancements': 1, 'continue': 1, 'shape': 1, 'future': 1, 'rapid': 1, 'pace': 1, 'innovation': 1, 'brought': 1, 'convenience': 1, 'efficiency': 1, 'daily': 1, 'lives': 1, 'however': 1, 'also': 1, 'raises': 1, 'ethical': 1, 'concerns': 1, 'challenges': 1, 'must': 1, 'addressed': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 2\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
        "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(\"Porter Stemmer Results:\", porter_stemmed)\n",
        "print(\"Lancaster Stemmer Results:\", lancaster_stemmed)\n",
        "print(\"Lemmatization Results:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFqugj5tyWBt",
        "outputId": "e18f4c4b-adc7-4436-be22-5aa19e6c8265"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer Results: ['technolog', 'revolution', 'way', 'live', 'work', 'commun', 'smartphon', 'artifici', 'intellig', 'advanc', 'technolog', 'continu', 'shape', 'futur', 'rapid', 'pace', 'innov', 'brought', 'conveni', 'effici', 'daili', 'live', 'howev', 'also', 'rais', 'ethic', 'concern', 'challeng', 'must', 'address']\n",
            "Lancaster Stemmer Results: ['technolog', 'revolv', 'way', 'liv', 'work', 'commun', 'smartphon', 'art', 'intellig', 'adv', 'technolog', 'continu', 'shap', 'fut', 'rapid', 'pac', 'innov', 'brought', 'conveny', 'efficy', 'dai', 'liv', 'howev', 'also', 'rais', 'eth', 'concern', 'challeng', 'must', 'address']\n",
            "Lemmatization Results: ['technology', 'revolutionized', 'way', 'live', 'work', 'communicate', 'smartphones', 'artificial', 'intelligence', 'advancement', 'technology', 'continue', 'shape', 'future', 'rapid', 'pace', 'innovation', 'brought', 'convenience', 'efficiency', 'daily', 'life', 'however', 'also', 'raise', 'ethical', 'concern', 'challenge', 'must', 'addressed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 3\n",
        "import re\n",
        "\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', text)\n",
        "\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "\n",
        "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']\n",
        "\n",
        "print(\"Words with more than 5 letters:\", long_words)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized Words:\", capitalized_words)\n",
        "print(\"Words containing only alphabets:\", alpha_words)\n",
        "print(\"Words starting with a vowel:\", vowel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-afpaAXylu8",
        "outputId": "04040734-5ef6-40d8-c971-b74695496cbb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with more than 5 letters: ['Technology', 'revolutionized', 'communicate', 'smartphones', 'artificial', 'intelligence', 'advancements', 'technology', 'continue', 'future', 'innovation', 'brought', 'convenience', 'efficiency', 'However', 'raises', 'ethical', 'concerns', 'challenges', 'addressed']\n",
            "Numbers: []\n",
            "Capitalized Words: ['Technology', 'From', 'The', 'However']\n",
            "Words containing only alphabets: ['Technology', 'has', 'revolutionized', 'the', 'way', 'we', 'live', 'work', 'and', 'communicate', 'From', 'smartphones', 'to', 'artificial', 'intelligence', 'advancements', 'in', 'technology', 'continue', 'to', 'shape', 'our', 'future', 'The', 'rapid', 'pace', 'of', 'innovation', 'has', 'brought', 'convenience', 'and', 'efficiency', 'to', 'our', 'daily', 'lives', 'However', 'it', 'also', 'raises', 'ethical', 'concerns', 'and', 'challenges', 'that', 'must', 'be', 'addressed']\n",
            "Words starting with a vowel: ['and', 'artificial', 'intelligence', 'advancements', 'in', 'our', 'of', 'innovation', 'and', 'efficiency', 'our', 'it', 'also', 'ethical', 'and', 'addressed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 4\n",
        "import re\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    tokens = re.findall(r\"[a-zA-Z0-9]+(?:'[a-z]+|-?[a-zA-Z0-9]+)*\", text)\n",
        "    return tokens\n",
        "\n",
        "text_with_placeholders = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
        "text_with_placeholders = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text_with_placeholders)\n",
        "text_with_placeholders = re.sub(r'\\+?\\d[\\d -]{8,}\\d', '<PHONE>', text_with_placeholders)\n",
        "\n",
        "tokens_custom = custom_tokenizer(text)\n",
        "\n",
        "print(\"Custom Tokens:\", tokens_custom)\n",
        "print(\"Text after Substitutions:\")\n",
        "print(text_with_placeholders)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toGn0PfkytTk",
        "outputId": "e44a849d-8a1a-4b4e-e9a8-8bfed8c5dd57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokens: ['Technology', 'has', 'revolutionized', 'the', 'way', 'we', 'live', 'work', 'and', 'communicate', 'From', 'smartphones', 'to', 'artificial', 'intelligence', 'advancements', 'in', 'technology', 'continue', 'to', 'shape', 'our', 'future', 'The', 'rapid', 'pace', 'of', 'innovation', 'has', 'brought', 'convenience', 'and', 'efficiency', 'to', 'our', 'daily', 'lives', 'However', 'it', 'also', 'raises', 'ethical', 'concerns', 'and', 'challenges', 'that', 'must', 'be', 'addressed']\n",
            "Text after Substitutions:\n",
            "Technology has revolutionized the way we live, work, and communicate. \n",
            "From smartphones to artificial intelligence, advancements in technology continue to shape our future. \n",
            "The rapid pace of innovation has brought convenience and efficiency to our daily lives. \n",
            "However, it also raises ethical concerns and challenges that must be addressed.\n"
          ]
        }
      ]
    }
  ]
}